# 深層学習（前編）

ラビットチャレンジ

m-takeda | 個人向けコース | 2021-06-22

# 深層学習day1

# １．入力層～中間層

## １－１．入力層

ニューラルネットワークでデータを最初に受け取る場所。

## １－２．ノード

データ１つ1つを受け取る部分をノードという。

## １－３．重み

重みは各々のデータをどの位使うべきかを決める。

## １－４．バイアス

入力の全体に対して、調整するためのパラメータ。

## １－５．総入力

入力データを重み、バイアスを通して変換した値。総入力を活性化関数に通すと出力（次のニューラルネットワークの入力）となる。

※確認テスト：中間層の総入力を計算しているソースコードは、「u2 = np.dot(z1, W2) + b2」


# ２．活性化関数

## ２－１．活性化関数

ニューラルネットワークにおいて、次の層への出力の大きさを決める非線形の関数。入力値の値によって、次の層への信号のON/OFFや強弱を定める働きをもつ。

※確認テスト：2次元のグラフで、直線で表現できない関数が非線形の関数

## ２－２．中間層用の活性化関数

- ReLU関数

- シグモイド（ロジスティック）関数

- ステップ関数

## ２－３．出力層用の活性化関数

- ソフトマックス関数

- 恒等写像

- シグモイド関数（ロジスティック関数）

## ２－４．活性化関数: ステップ関数

しきい値を超えたら発火する関数であり、出力は常に１か０。パーセプトロン（ニューラルネットワークの前身）で利用された関数。課題0 -1間の間を表現できず、線形分離可能なものしか学習できなかった。

## ２－５．活性化関数: シグモイド関数

0 ~ 1の間を緩やかに変化する関数で、ステップ関数ではON/OFFしかない状態に対し、信号の強弱を伝えられるようになり、予想ニューラルネットワーク普及のきっかけとなった。課題大きな値では出力の変化が微小なため、勾配消失問題を引き起こす事があった。

## ２－６．活性化関数: ReLU関数

今最も使われている活性化関数勾配消失問題の回避とスパース化に貢献することで良い成果をもたらしている。

※確認テスト：活性化関数を通している計算しているソースコードは、「z1 = functions.relu(u1)」

※演習：
![1_1_forward_propagation](img/Web_capture_16-6-2021_12215_localhost.jpeg)


# ３．出力層

## ３－１．誤差関数

誤差関数：ニューラルネットワークの出力と正解を比べて、一致具合を数値化する。

※確認テスト：2乗するのは符号の影響を除くため、1/2するのは計算式の単純化のため。

・２乗誤差関数： 回帰問題で使用される。「loss = functions.mean\_squared\_error(d, y)」

・交差エントロピー：分類問題で使用される。「loss = functions.cross\_entropy\_error(d, y)」

## ３－２．出力層の活性化関数

学習結果である中間層の出力を利用者が扱える形に変化する。問題に応じて、使用する関数が使い分けられる。

・恒等写像：回帰問題で使用される。何もしない関数。

・シグモイド関数：２値分類で使用される。

・ソフトマックス関数：多クラス分類で使用される。

※確認テスト：ソフトマックス関数のソースコード

def softmax(x):

    if x.ndim == 2: # ミニバッチの場合

        x = x.T #

        x = x - np.max(x, axis=0) # オーバーフロー対策

        y = np.exp(x) / np.sum(np.exp(x), axis=0) # 列方向でソフトマックス関数の式を実行

        return y.T #

    x = x - np.max(x) # オーバーフロー対策

    return np.exp(x) / np.sum(np.exp(x)) # ソフトマックス関数の計算式本体

※確認テスト：交差エントロピー関数のソースコード

def cross\_entropy\_error(d, y):

    if y.ndim == 1: # ミニバッチの場合

        d = d.reshape(1, d.size) #

        y = y.reshape(1, y.size) #

    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換(不正解のラベルは０になる=誤差は0)

    if d.size == y.size: #

        d = d.argmax(axis=1) #

    batch_size = y.shape[0] #

    return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size # 交差エントロピー関数の計算式本体


# ４．勾配降下法

## ４－１．勾配降下法

誤差を最小化するパラメータを発見する

- 学習率の値によって学習の効率が大きく異なる
- 学習率が大きすぎた場合、最小値にいつまでもたどり着かず発散してしまう
- 学習率が小さい場合、発散することはないが時間がかかってしまう

## ４－２．確率的勾配降下法（SGD)

ランダムに抽出したサンプルの誤差（勾配降下法は全サンプルの平均誤差）

確率的勾配降下法のメリット

・データが冗長な場合の計算コストの軽減

・望まない局所極小解に収束するリスクの軽減

・オンライン学習ができる

※確認テスト：オンライン学習とは「データ１件ごとに学習を行うこと」

## ４－３．ミニバッチ勾配降下法

ランダムに分割したデータの集合に属するサンプルの平均誤差

ミニバッチ勾配降下法のメリット

・確率的勾配降下法のメリットを損なわず、計算機の計算資源を有効利用できる→CPUを利用したスレッド並列化やGPUを利用したSIMD並列化算出された誤差を、出力層側から順に微分し、前の層前の層へと伝播。最小限の計算で各パラメータでの微分値を解析的に計算する手法

※確認テスト：
「次のエポックの重み」　←　「今回の誤差」× 学習率　→　「今回のエポックの重み」


# ５．誤差逆伝播法

## ５－１．誤差逆伝播法

算出された誤差を、出力層側から順に微分し、各層ごとに、前の層前の層へと伝播して、誤差勾配を計算する

最小限の（再帰的な計算を避けた）計算で各パラメータでの微分値を（連鎖律を利用して）解析的に計算する手法（数値微分では計算量が多い）

※確認テスト：

delta2 = functions.d\_mean\_squared\_error(d, y) ＃ 出力層の微分を

delta1 = np.dot(delta2, W2.T) \* functions.d\_sigmoid(z1) # 前の層の微分に使用

※確認テスト：

delta2 = functions.d\_mean\_squared\_error(d, y) ＃ 活性化関数は恒等写像

grad[&#39;W2&#39;] = np.dot(z1.T, delta2) # W2の傾き

※演習：
![1_2_back_propagation](img/Web_capture_16-6-2021_122412_localhost.jpeg)


# ６．ニューラルネットワーク作成時の留意事項

## ６－1．入力層の設計

入力層として取るべきでないデータ

- 欠損値が多いデータ
- 誤差の大きいデータ
- 出力そのもの、出力を加工した情報
- 連続性の無いデータ
- 無意味な数が割り当てられているデータ

## ６－２．過学習

一般に、パラメータの多い巨大なニューラルネットワークで発生する現象。

過学習を見分けるには、訓練用のデータとテスト用データに分けて、訓練用データで学習した後のテスト用データでの成績が悪くなることで判断することができる。

## ６－３．データ集合の拡張

画像認識に効果が高い。

様々な変換（オフセット、ノイズ、ドロップアウト、JPEG圧縮、ぼかし、色温度、ジグゾー、拡大縮小、回転、剪断、回転９０度、クロップ）を組み合わせて水増しデータを生成する。

変換の結果が、データセット内で混同するデータが発生しないようにする。

中間層へのノイズ注入で様々な抽象化レベルでのデータ拡張ができる。

## ６－４．ＣＮＮで扱えるデータの種類

次元間で繋がりのあるデータ（音声、カラー画像、動画）

## ６－５．特徴量の転移

特徴量の抽出済みのベースモデルを使用する。

転移学習：ベースモデルの重みは固定する（更新しない）。

ファインチューニング：ベースモデルの重みを再学習する。


# 深層学習day2

# １．勾配消失問題

## １－１．活性化関数の選択

ReLU関数が今最も使われている活性化関数。

勾配消失問題の回避とスパース化に貢献することで良い成果をもたらしている（シグモイド関数では値が小さくなる）。

## １－２．初期値の設定方法

## １－２－１．重みの初期値設定-Xavier

・初期値を設定するための活性化関数には下記の関数を用いる。

- ReLU関数
- シグモイド（ロジスティック）関数
- 双曲線正接関数

・Xavierの初期値の設定方法：重みの要素を、前の層のノード数の平方根で除算した値。

## １－２－２．重みの初期値設定-He

・Heの初期値を設定する際の活性化関数にはReLU関数を用いる。

・初期値の設定方法：重みの要素を、前の層のノード数の平方根で除算した値に対し√２をかけ合わせた値。

※確認テスト：重みの初期値をゼロに設置すると、学習によって調整する機能がなくなる。

## １－３．バッチ正規化

ミニバッチ単位で、入力値のデータ偏りを抑制する手法。
活性化関数に値を渡す前後に、バッチ正規化の処理を孕んだ層を加える。

※確認テスト：バッチ正規化の効果

・勾配消失を抑制する。

・あらかじめデータを整えることになり、計算が速くなる。

※演習：
![2_2_1_vanishing_gradient](img/Web_capture_16-6-2021_122820_localhost.jpeg)


# ２．学習率最適化手法

## ２－１．モメンタム

誤差をパラメータで微分したものと学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する。

・局所的最適解にはならず、大域的最適解となる。

・谷間についてから最も低い位置(最適値)にいくまでの時間が早い。

## ２－２．ＡｄａＧｒａｄ

誤差をパラメータで微分したものと再定義した学習率の積を減算する。

・勾配の緩やかな斜面に対して、最適値に近づける。

・学習率が徐々に小さくなるので、鞍点問題を引き起こす事があった。

## ２－３．ＲＭＳＰｒｏｐ

誤差をパラメータで微分したものと再定義した学習率の積を減算する。

・局所的最適解にはならず、大域的最適解となる。

・ハイパーパラメータの調整が必要な場合が少ない。

## ２－４．Ａｄａｍ

・モメンタムの、過去の勾配の指数関数的減衰平均。

・RMSPropの、過去の勾配の2乗の指数関数的減衰平均。

上記をそれぞれ孕んだ最適化アルゴリズムである。

※演習：
![2_4_optimizer](img/Web_capture_22-6-2021_122354_localhost.jpeg)

[try] 学習率を変えてみよう

・learning_rate = 0.09に変更

[try] 活性化関数と重みの初期化方法を変更して違いを見てみよう

・activationはReLU、weight_init_stdは'He'に変更

[try] バッチ正規化をして変化を見てみよう

・use_batchnormをTrueに変更
 
![2_4_optimizer](img/Web_capture_21-6-2021_123438_localhost.jpeg)


# ３．過学習

## ３－１．過学習

学習データにフィットし過ぎることで、汎化性能が落ちること。

テスト誤差と訓練誤差との学習曲線が乖離する。

大きなニューラルネットワークほど発生しやすい。

重みが大きい値となることで起こる過学習の解決策として、誤差に対して正則化項を加えることで、重みを抑制する。

## ３－２．L1正則化、L2正則化

ネットワークの自由度を制約すること。

誤差関数にノルムを加える。

誤差に対して正則化項を加えることで重みを抑える。誤差をパラメータで微分したものと学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する。

※例題：Ｌ２パラメータの正則化

def ridge(param,grad,rate):

	param: target paramater
	grad : gradients to param
	rate : ridge coefficient
	grad += rate * param

・演習ソース（2_5_overfiting）では

    grad['W' + str(idx)] = network.layers['Affine' + str(idx)].dW + weight_decay_lambda * network.params['W' + str(idx)]
    weight_decay += 0.5 * weight_decay_lambda * np.sqrt(np.sum(network.params['W' + str(idx)] ** 2))

※例題：Ｌ１パラメータの正則化

def lasso(param,grad,rate):

	param: target paramater
	grad : gradients to param
	rate : lasso coefficient
	x = sign(param)
	grad += rate * x

・演習ソース（2_5_overfiting）では

    grad['W' + str(idx)] = network.layers['Affine' + str(idx)].dW + weight_decay_lambda * np.sign(network.params['W' + str(idx)])
    weight_decay += weight_decay_lambda * np.sum(np.abs(network.params['W' + str(idx)]))

※例題：データ集合の拡張

def randge_crop(image,crop_size)

	image:(hight,width,channel)
	crop_size:(crop_hight,crop_width)
	hight >= crop_hight,width >= crop_width
	h, w, _ = image.shape
	crop_h, crop_w = crop_size
	top = np.random.randint(0, h - crop_h)
	left = np.random.randint(0, w - crop_w)
	bottom = top + crop_h
	right = left + crop_w
	image = image[bottom:top,right:left,:]
	return image



※確認テスト：リッジ回帰（L2正則化）はハイパーパラメータを大きくすると重みがゼロに近づく

※確認テスト：L1正則化をグラフで表すと、正則化項がスパース推定の方形で表現される

## ３－３．ドロップアウト

過学習の課題：ノードの数が多いこと。

ドロップアウトとは：ランダムにノードを削除して学習させること（データ量を変化させずに、異なるモデルを学習させていると解釈できる）。

※演習：
![2_5_overfiting](img/Web_capture_18-6-2021_12278_localhost.jpeg)

[try] weight_decay_lambdaの値を変更して正則化の強さを確認しよう

・weight_decay_lambda = 0.008

[try] dropout_ratioの値を変更してみよう

・dropout_ratio = 0.19

[try] optimizerとdropout_ratioの値を変更してみよう

・optimizer = optimizer.Momentum(learning_rate=0.01, momentum=0.9)
![2_5_overfiting](img/Web_capture_23-6-2021_123431_localhost.jpeg)

・optimizer = optimizer.AdaGrad(learning_rate=0.01)
![2_5_overfiting](img/Web_capture_23-6-2021_122734_localhost.jpeg)

・optimizer = optimizer.Adam()
![2_5_overfiting](img/Web_capture_23-6-2021_121431_localhost.jpeg)


# ４．畳み込みニューラルネットワークの概念

・「畳み込み層」「プーリング層」を構造に含み、画像分野のソリューションで効果を発揮している。

## ４－１．畳み込み層

フィルター（全結合層の重みに相当）により特徴を抽出する・

## ４－１－１．バイアス

バイアスの値は1つだけであり、同じ値がフィルター適用後のすべての要素に加算される。

## ４－１－２．パディング

データの周辺を固定値で埋めて、畳み込み処理でのデータ縮小を抑制する（端の特徴を捉える）。

## ４－１－３．ストライド

畳み込み処理でのフィルターの移動量の大きさ。

## ４－１－４．チャンネル

データの奥行方向の層数。

## ４－２．プーリング層

・MAXプーリング：特徴マップの解像度を下げるとき、対象領域の「最大値」を採用する。

・AVRプーリング：特徴マップの解像度を下げるとき、対象領域の「平均値」を採用する。

※確認テスト：
サイズ６×６の入力画像をサイズ２×２のフィルタでストライド１、パディング１の畳み込み処理で７×７の出力画像になる。
（各次元ごとに：O=(I＋２P-F)/S+1）

※演習：

・関数内でtransposeの処理をしている行をコメントアウトして下のコードを実行してみよう
![2_6_simple_convolution_network](img/Web_capture_19-6-2021_21947_localhost.jpeg)

・im2colの確認で出力したcolをimageに変換して確認しよう

![2_6_simple_convolution_network](img/Web_capture_22-6-2021_124651_localhost.jpeg)

# ５．最新のＣＮＮ

## ５－１．ＡｌｅｘＮｅｔ

・5層の畳み込み層およびプーリング層など、それに続く3層の全結合層から構成される。

・過学習対策としてサイズ4096の全結合層の出力にドロップアウトを使用している。

AlexNetとは2012年に開かれた画像認識コンペティション2位に大差をつけて優勝したモデルである。AlexNetの登場で、ディープラーニングが大きく注目を集めた。
